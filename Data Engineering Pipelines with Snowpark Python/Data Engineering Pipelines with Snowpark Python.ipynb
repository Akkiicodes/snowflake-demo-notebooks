{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell1"
      },
      "source": "# Data Engineering Pipelines with Snowpark Python\n\n\n\"Data engineers are focused primarily on building and maintaining data pipelines that transport data through different steps and put it into a usable state ... The data engineering process encompasses the overall effort required to create **data pipelines** that automate the transfer of data from place to place and transform that data into a specific format for a certain type of analysis. In that sense, data engineering isn\u2019t something you do once. It\u2019s an ongoing practice that involves collecting, preparing, transforming, and delivering data. A data pipeline helps automate these tasks so they can be reliably repeated. It\u2019s a practice more than a specific technology.\" (From Cloud Data Engineering for Dummies, Snowflake Special Edition)\n\nAre you interested in unleashing the power of Snowpark Python to build data engineering pipelines? Well then, this Quickstart is for you! The focus here will be on building data engineering pipelines with Python, and not on data science. For examples of doing data science with Snowpark Python please check out our [Machine Learning with Snowpark Python: - Credit Card Approval Prediction](https://quickstarts.snowflake.com/guide/getting_started_snowpark_machine_learning/index.html?index=..%2F..index#0) Quickstart.\n\n[This Quickstart](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_python/index.html) will cover a lot of ground, and by the end you will have built a robust data engineering pipeline using Snowpark Python stored procedures. That pipeline will process data incrementally, be orchestrated with Snowflake tasks, and be deployed via a CI/CD pipeline. You'll also learn how to use Snowflake's new developer CLI tool and Visual Studio Code extension! Here's a quick visual overview:",
      "id": "152b9c5c-1d6a-4792-9f21-cb198d9554cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell2"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "st.image(\"https://raw.githubusercontent.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python/main/images/demo_overview.png\",width=800)"
      ],
      "id": "63cfc92d-977c-4019-a64e-a297b49b356a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell3"
      },
      "source": "### Prerequisites\n* Familiarity with Python\n* Familiarity with the DataFrame API\n* Familiarity with Snowflake\n\n### What You\u2019ll Learn\nYou will learn about the following Snowflake features during this Quickstart:\n\n* Snowflake's Table Format\n* Data ingestion with COPY\n* Schema inference\n* Data sharing/marketplace (instead of ETL)\n* Streams for incremental processing (CDC)\n* Streams on views\n* Python UDFs (with third-party packages)\n* Python Stored Procedures\n* Snowpark DataFrame API\n* Snowpark Python programmability\n* Warehouse elasticity (dynamic scaling)\n* SnowCLI (PuPr)\n* Tasks (with Stream triggers)\n* Task Observability\n\n### What You\u2019ll Need\nYou will need the following things before beginning:\n\n* Snowflake account\n    * **A Snowflake Account**\n    * **A Snowflake user created with ACCOUNTADMIN permissions**. This user will be used to get things setup in Snowflake.\n    * **Anaconda Terms & Conditions accepted**. See Getting Started section in [Third-Party Packages](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#getting-started).\n\n### What You\u2019ll Build\nDuring this Quickstart you will accomplish the following things:\n\n* Load Parquet data to Snowflake using schema inference\n* Setup access to Snowflake Marketplace data\n* Create a Python UDF to convert temperature\n* Create a data engineering pipeline with Python stored procedures to incrementally process data\n* Orchestrate the pipelines with tasks\n* Monitor the pipelines with Snowsight\n* Deploy the Snowpark Python stored procedures via a CI/CD pipeline\n\n\n## Setup Snowflake\n\nYou can run SQL queries against Snowflake in many different ways (through the Snowsight UI, SnowSQL, etc.) but for this Quickstart we'll be using the Snowflake Notebooks.\n\n### Run the Script\nTo set up all the objects we'll need in Snowflake for this Quickstart you'll need to run following steps.",
      "id": "5a796a19-195f-488b-8a54-240de881582c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell4"
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Step #1: Accept Anaconda Terms & Conditions\n-- ----------------------------------------------------------------------------\n\n-- See Getting Started section in Third-Party Packages (https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#getting-started)\n\n\n-- ----------------------------------------------------------------------------\n-- Step #2: Create the account level objects\n-- ----------------------------------------------------------------------------\nUSE ROLE ACCOUNTADMIN;\n\n-- Roles\nSET MY_USER = CURRENT_USER(); \nCREATE OR REPLACE ROLE HOL_ROLE;\nGRANT ROLE HOL_ROLE TO ROLE SYSADMIN;\nGRANT ROLE HOL_ROLE TO USER IDENTIFIER($MY_USER);\n\nCREATE STAGE IF NOT EXISTS SCRIPTS \n\tDIRECTORY = ( ENABLE = true );\n\nGRANT EXECUTE TASK ON ACCOUNT TO ROLE HOL_ROLE;\nGRANT MONITOR EXECUTION ON ACCOUNT TO ROLE HOL_ROLE;\nGRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE HOL_ROLE;\n\n-- Databases\nCREATE OR REPLACE DATABASE NB_HOL_DB;\nGRANT OWNERSHIP ON DATABASE NB_HOL_DB TO ROLE HOL_ROLE;\n\n-- Warehouses\nCREATE OR REPLACE WAREHOUSE HOL_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\nGRANT OWNERSHIP ON WAREHOUSE HOL_WH TO ROLE HOL_ROLE;",
      "id": "7551d60d-f4e1-44cb-be72-ce26d4814c50"
    },
    {
      "cell_type": "code",
      "id": "b706ef72-370b-41c4-a46f-bb3eb8f58f45",
      "metadata": {
        "language": "sql",
        "name": "cell5",
        "collapsed": false,
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Step #3: Create the database level objects\n-- ----------------------------------------------------------------------------\nUSE ROLE HOL_ROLE;\nUSE WAREHOUSE HOL_WH;\nUSE DATABASE NB_HOL_DB;\n\n-- Schemas\nCREATE OR REPLACE SCHEMA EXTERNAL;\nCREATE OR REPLACE SCHEMA RAW_POS;\nCREATE OR REPLACE SCHEMA RAW_CUSTOMER;\nCREATE OR REPLACE SCHEMA HARMONIZED;\nCREATE OR REPLACE SCHEMA ANALYTICS;\n\n-- External Frostbyte objects\nUSE SCHEMA EXTERNAL;\nCREATE OR REPLACE FILE FORMAT PARQUET_FORMAT\n    TYPE = PARQUET\n    COMPRESSION = SNAPPY\n;\nCREATE OR REPLACE STAGE FROSTBYTE_RAW_STAGE\n    URL = 's3://sfquickstarts/data-engineering-with-snowpark-python/'\n;",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell6"
      },
      "source": "### Load Raw \n\nDuring this step we will be loading the raw Tasty Bytes POS and Customer loyalty data from raw Parquet files in `s3://sfquickstarts/data-engineering-with-snowpark-python/` to our `RAW_POS` and `RAW_CUSTOMER` schemas in Snowflake. And you are going to be orchestrating this process from your laptop in Python using the Snowpark Python API. To put this in context, we are on step **#2** in our data flow overview:\n",
      "id": "a57b699e-0912-4f0c-b23b-7a3ddf42a93a"
    },
    {
      "cell_type": "code",
      "id": "a55a0d00-69d2-4aae-9c68-755d5c30fb4c",
      "metadata": {
        "language": "python",
        "name": "cell7",
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import streamlit as st\nst.image(\"https://raw.githubusercontent.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python/main/images/demo_overview.png\",width=800)",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "edba6e43-badd-416f-9a62-7cf77cf248b1",
      "metadata": {
        "name": "cell8"
      },
      "source": "To load the raw data, execute the following:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell9"
      },
      "outputs": [],
      "source": "import time\nfrom snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\n#import snowflake.snowpark.functions as F\n\n\nPOS_TABLES = ['country', 'franchise', 'location', 'menu', 'truck', 'order_header', 'order_detail']\nCUSTOMER_TABLES = ['customer_loyalty']\nTABLE_DICT = {\n    \"pos\": {\"schema\": \"RAW_POS\", \"tables\": POS_TABLES},\n    \"customer\": {\"schema\": \"RAW_CUSTOMER\", \"tables\": CUSTOMER_TABLES}\n}\n\n# SNOWFLAKE ADVANTAGE: Schema detection\n# SNOWFLAKE ADVANTAGE: Data ingestion with COPY\n# SNOWFLAKE ADVANTAGE: Snowflake Tables (not file-based)\n\ndef load_raw_table(session, tname=None, s3dir=None, year=None, schema=None):\n    session.use_schema(schema)\n    if year is None:\n        location = \"@external.frostbyte_raw_stage/{}/{}\".format(s3dir, tname)\n    else:\n        print('\\tLoading year {}'.format(year)) \n        location = \"@external.frostbyte_raw_stage/{}/{}/year={}\".format(s3dir, tname, year)\n    \n    # we can infer schema using the parquet read option\n    df = session.read.option(\"compression\", \"snappy\") \\\n                            .parquet(location)\n    df.copy_into_table(\"{}\".format(tname))\n\n# SNOWFLAKE ADVANTAGE: Warehouse elasticity (dynamic scaling)\n\ndef load_all_raw_tables(session):\n    _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\n    for s3dir, data in TABLE_DICT.items():\n        tnames = data['tables']\n        schema = data['schema']\n        for tname in tnames:\n            print(\"Loading {}\".format(tname))\n            # Only load the first 3 years of data for the order tables at this point\n            # We will load the 2022 data later in the lab\n            if tname in ['order_header', 'order_detail']:\n                for year in ['2019']:#, '2020', '2021']: \n                    load_raw_table(session, tname=tname, s3dir=s3dir, year=year, schema=schema)\n            else:\n                load_raw_table(session, tname=tname, s3dir=s3dir, schema=schema)\n\n    _ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n\ndef validate_raw_tables(session):\n    # check column names from the inferred schema\n    for tname in POS_TABLES:\n        print('{}: \\n\\t{}\\n'.format(tname, session.table('RAW_POS.{}'.format(tname)).columns))\n\n    for tname in CUSTOMER_TABLES:\n        print('{}: \\n\\t{}\\n'.format(tname, session.table('RAW_CUSTOMER.{}'.format(tname)).columns))",
      "id": "bdbbe721-66fc-4031-b3fe-6f195ee5baa7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell10"
      },
      "outputs": [],
      "source": [
        "# Add the utils package to our path and import the snowpark_utils function\n",
        "import os, sys\n",
        "current_dir = os.getcwd()\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "sys.path.append(parent_dir)"
      ],
      "id": "f6659e70-913c-4978-8b2e-d154843328d7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
      "id": "52cf12e2-9257-47b0-8c55-c7d87cce6267"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "load_all_raw_tables(session)"
      ],
      "id": "f624c8ad-f94b-4cf2-b06d-76b32b39a507"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": [
        "validate_raw_tables(session)"
      ],
      "id": "62214d74-b02d-4868-8ad2-e6967d581ab0"
    },
    {
      "cell_type": "markdown",
      "id": "9c058d54-6ad1-429b-813b-d8309a433f7a",
      "metadata": {
        "name": "cell14",
        "collapsed": false
      },
      "source": "### Viewing What Happened in Snowflake\nThe [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity.html#query-history) in Snowflake is a very power feature, that logs every query run against your Snowflake account, no matter which tool or process initiated it. And this is especially helpful when working with client tools and APIs.\n\nThe Python script you just ran did a small amount of work locally, basically just orchestrating the process by looping through each table and issuing the command to Snowflake to load the data. But all of the heavy lifting ran inside Snowflake! This push-down is a hallmark of the Snowpark API and allows you to leverage the scalability and compute power of Snowflake!\n\nLog in to your Snowflake account and take a quick look at the SQL that was generated by the Snowpark API. This will help you better understand what the API is doing and will help you debug any issues you may run into.\n\n\n### Schema Inference\nOne very helpful feature in Snowflake is the ability to infer the schema of files in a stage that you wish to work with. This is accomplished in SQL with the [`INFER_SCHEMA()`](https://docs.snowflake.com/en/sql-reference/functions/infer_schema.html) function. The Snowpark Python API does this for you automatically when you call the `session.read()` method. Here is the code snippet:\n\n```python\n    # we can infer schema using the parquet read option\n    df = session.read.option(\"compression\", \"snappy\") \\\n                            .parquet(location)\n```\n\n### Data Ingestion with COPY\nIn order to load the data into a Snowflake table we will use the `copy_into_table()` method on a DataFrame. This method will create the target table in Snowflake using the inferred schema (if it doesn't exist), and then call the highly optimized Snowflake [`COPY INTO &lt;table&gt;` Command](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html). Here is the code snippet:\n\n```python\n    df.copy_into_table(\"{}\".format(tname))\n```\n\n### Snowflake's Table Format\nOne of the major advantages of Snowflake is being able to eliminate the need to manage a file-based data lake. And Snowflake was designed for this purpose from the beginning. In the step we are loading the raw data into a structured Snowflake managed table. But Snowflake tables can natively support structured and semi-structured data, and are stored in Snowflake's mature cloud table format (which predates Hudi, Delta or Iceberg).\n\nOnce loaded into Snowflake the data will be securely stored and managed, without the need to worry about securing and managing raw files. Additionally the data, whether raw or structured, can be transformed and queried in Snowflake using SQL or the language of your choice, without needing to manage separate compute services like Spark.\n\nThis is a huge advantage for Snowflake customers.\n\n\n### Warehouse Elasticity (Dynamic Scaling)\nWith Snowflake there is only one type of user defined compute cluster, the [Virtual Warehouse](https://docs.snowflake.com/en/user-guide/warehouses.html), regardless of the language you use to process that data (SQL, Python, Java, Scala, Javascript, etc.). This makes working with data much simpler in Snowflake. And governance of the data is completely separated from the compute cluster, in other words there is no way to get around Snowflake governance regardless of the warehouse settings or language being used.\n\nAnd these virtual warehouses can be dynamically scaled, in under a second for most sized warehouses! This means that in your code you can dynamically resize the compute environment to increase the amount of capacity to run a section of code in a fraction of the time, and then dynamically resized again to reduce the amount of capacity. And because of our per-second billing (with a sixty second minimum) you won't pay any extra to run that section of code in a fraction of the time!\n\nLet's see how easy that is done. Here is the code snippet:\n\n```python\n_ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE\").collect()\n\n# Some data processing code\n\n_ = session.sql(\"ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL\").collect()\n```\n\nPlease also note that we included the `WAIT_FOR_COMPLETION` parameter in the first `ALTER WAREHOUSE` statement. Setting this parameter to `TRUE` will block the return of the `ALTER WAREHOUSE` command until the resize has finished provisioning all its compute resources. This way we make sure that the full cluster is available before processing any data with it.\n\nWe will use this pattern a few more times during this Quickstart, so it's important to understand.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell15"
      },
      "source": "## Load Weather\n\n\nDuring this step we will be \"loading\" the raw weather data to Snowflake. But \"loading\" is the really the wrong word here. Because we're using Snowflake's unique data sharing capability we don't actually need to copy the data to our Snowflake account with a custom ETL process. Instead we can directly access the weather data shared by Weather Source in the Snowflake Data Marketplace. To put this in context, we are on step **#3** in our data flow overview. \n\n\n### Snowflake Data Marketplace\nWeather Source is a leading provider of global weather and climate data and their OnPoint Product Suite provides businesses with the necessary weather and climate data to quickly generate meaningful and actionable insights for a wide range of use cases across industries. Let's connect to the `Weather Source LLC: frostbyte` feed from Weather Source in the Snowflake Data Marketplace by following these steps:\n\n* Login to Snowsight\n* Click on the `Marketplace` link in the left navigation bar\n* Enter \"Weather Source LLC: frostbyte\" in the search box and click return\n* Click on the \"Weather Source LLC: frostbyte\" listing tile\n* Click the blue \"Get\" button\n    * Expand the \"Options\" dialog\n    * Change the \"Database name\" to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n    * Select the \"HOL_ROLE\" role to have access to the new database\n* Click on the blue \"Get\" button\n\nThat's it... we don't have to do anything from here to keep this data updated. The provider will do that for us and data sharing means we are always seeing whatever they have published. How amazing is that? Just think of all the things you didn't have do here to get access to an always up-to-date, third-party dataset!",
      "id": "64122344-5484-430d-907a-2ba3702d7172"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell16"
      },
      "outputs": [],
      "source": [
        "-- Make sure you grant the table privileges to HOL_ROLE for this to run\n",
        "SELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100; "
      ],
      "id": "46b8555e-4c29-4d5c-8193-d182c822baf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell17"
      },
      "source": "## Create POS View\n\nDuring this step we will be creating a view to simplify the raw POS schema by joining together 6 different tables and picking only the columns we need. But what's really cool is that we're going to define that view with the Snowpark DataFrame API! Then we're going to create a Snowflake stream on that view so that we can incrementally process changes to any of the POS tables. To put this in context, we are on step **#4** in our data flow overview.\n\n### Run the Script\nTo create the view and stream, execute the following code.",
      "id": "9192a8d0-66fb-4a73-9883-531d25607a9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": "# SNOWFLAKE ADVANTAGE: Snowpark DataFrame API\n# SNOWFLAKE ADVANTAGE: Streams for incremental processing (CDC)\n# SNOWFLAKE ADVANTAGE: Streams on views\n\n\nfrom snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef create_pos_view(session):\n    session.use_schema('HARMONIZED')\n    order_detail = session.table(\"RAW_POS.ORDER_DETAIL\").select(F.col(\"ORDER_DETAIL_ID\"), \\\n                                                                F.col(\"LINE_NUMBER\"), \\\n                                                                F.col(\"MENU_ITEM_ID\"), \\\n                                                                F.col(\"QUANTITY\"), \\\n                                                                F.col(\"UNIT_PRICE\"), \\\n                                                                F.col(\"PRICE\"), \\\n                                                                F.col(\"ORDER_ID\"))\n    order_header = session.table(\"RAW_POS.ORDER_HEADER\").select(F.col(\"ORDER_ID\"), \\\n                                                                F.col(\"TRUCK_ID\"), \\\n                                                                F.col(\"ORDER_TS\"), \\\n                                                                F.to_date(F.col(\"ORDER_TS\")).alias(\"ORDER_TS_DATE\"), \\\n                                                                F.col(\"ORDER_AMOUNT\"), \\\n                                                                F.col(\"ORDER_TAX_AMOUNT\"), \\\n                                                                F.col(\"ORDER_DISCOUNT_AMOUNT\"), \\\n                                                                F.col(\"LOCATION_ID\"), \\\n                                                                F.col(\"ORDER_TOTAL\"))\n    truck = session.table(\"RAW_POS.TRUCK\").select(F.col(\"TRUCK_ID\"), \\\n                                                F.col(\"PRIMARY_CITY\"), \\\n                                                F.col(\"REGION\"), \\\n                                                F.col(\"COUNTRY\"), \\\n                                                F.col(\"FRANCHISE_FLAG\"), \\\n                                                F.col(\"FRANCHISE_ID\"))\n    menu = session.table(\"RAW_POS.MENU\").select(F.col(\"MENU_ITEM_ID\"), \\\n                                                F.col(\"TRUCK_BRAND_NAME\"), \\\n                                                F.col(\"MENU_TYPE\"), \\\n                                                F.col(\"MENU_ITEM_NAME\"))\n    franchise = session.table(\"RAW_POS.FRANCHISE\").select(F.col(\"FRANCHISE_ID\"), \\\n                                                        F.col(\"FIRST_NAME\").alias(\"FRANCHISEE_FIRST_NAME\"), \\\n                                                        F.col(\"LAST_NAME\").alias(\"FRANCHISEE_LAST_NAME\"))\n    location = session.table(\"RAW_POS.LOCATION\").select(F.col(\"LOCATION_ID\"))\n\n    \n    '''\n    We can do this one of two ways: either select before the join so it is more explicit, or just join on the full tables.\n    The end result is the same, it's mostly a readibility question.\n    '''\n    # order_detail = session.table(\"RAW_POS.ORDER_DETAIL\")\n    # order_header = session.table(\"RAW_POS.ORDER_HEADER\")\n    # truck = session.table(\"RAW_POS.TRUCK\")\n    # menu = session.table(\"RAW_POS.MENU\")\n    # franchise = session.table(\"RAW_POS.FRANCHISE\")\n    # location = session.table(\"RAW_POS.LOCATION\")\n\n    t_with_f = truck.join(franchise, truck['FRANCHISE_ID'] == franchise['FRANCHISE_ID'], rsuffix='_f')\n    oh_w_t_and_l = order_header.join(t_with_f, order_header['TRUCK_ID'] == t_with_f['TRUCK_ID'], rsuffix='_t') \\\n                                .join(location, order_header['LOCATION_ID'] == location['LOCATION_ID'], rsuffix='_l')\n    final_df = order_detail.join(oh_w_t_and_l, order_detail['ORDER_ID'] == oh_w_t_and_l['ORDER_ID'], rsuffix='_oh') \\\n                            .join(menu, order_detail['MENU_ITEM_ID'] == menu['MENU_ITEM_ID'], rsuffix='_m')\n    final_df = final_df.select(F.col(\"ORDER_ID\"), \\\n                            F.col(\"TRUCK_ID\"), \\\n                            F.col(\"ORDER_TS\"), \\\n                            F.col('ORDER_TS_DATE'), \\\n                            F.col(\"ORDER_DETAIL_ID\"), \\\n                            F.col(\"LINE_NUMBER\"), \\\n                            F.col(\"TRUCK_BRAND_NAME\"), \\\n                            F.col(\"MENU_TYPE\"), \\\n                            F.col(\"PRIMARY_CITY\"), \\\n                            F.col(\"REGION\"), \\\n                            F.col(\"COUNTRY\"), \\\n                            F.col(\"FRANCHISE_FLAG\"), \\\n                            F.col(\"FRANCHISE_ID\"), \\\n                            F.col(\"FRANCHISEE_FIRST_NAME\"), \\\n                            F.col(\"FRANCHISEE_LAST_NAME\"), \\\n                            F.col(\"LOCATION_ID\"), \\\n                            F.col(\"MENU_ITEM_ID\"), \\\n                            F.col(\"MENU_ITEM_NAME\"), \\\n                            F.col(\"QUANTITY\"), \\\n                            F.col(\"UNIT_PRICE\"), \\\n                            F.col(\"PRICE\"), \\\n                            F.col(\"ORDER_AMOUNT\"), \\\n                            F.col(\"ORDER_TAX_AMOUNT\"), \\\n                            F.col(\"ORDER_DISCOUNT_AMOUNT\"), \\\n                            F.col(\"ORDER_TOTAL\"))\n    final_df.create_or_replace_view('POS_FLATTENED_V')\n\ndef create_pos_view_stream(session):\n    session.use_schema('HARMONIZED')\n    _ = session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n                        ON VIEW POS_FLATTENED_V \\\n                        SHOW_INITIAL_ROWS = TRUE').collect()\n\ndef test_pos_view(session):\n    session.use_schema('HARMONIZED')\n    tv = session.table('POS_FLATTENED_V')\n    tv.limit(5).show()",
      "id": "8235cfa2-7694-4d77-bbf2-7d3c71c45ab1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell19"
      },
      "outputs": [],
      "source": [
        "create_pos_view(session)"
      ],
      "id": "8f14352d-26ee-40a6-b1f5-0d94ffaa302a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell20"
      },
      "outputs": [],
      "source": [
        "create_pos_view_stream(session)"
      ],
      "id": "78c9076f-0620-4476-a94f-8d2b5897a2d6"
    },
    {
      "cell_type": "markdown",
      "id": "419aa2e8-87c8-446f-8236-c8dd2e2440c3",
      "metadata": {
        "name": "cell21"
      },
      "source": "### Snowpark DataFrame API\nThe first thing you'll notice in the `create_pos_view()` function is that we define the Snowflake view using the Snowpark DataFrame API. After defining the final DataFrame, which captures all the logic we want in the view, we can simply call the Snowpark `create_or_replace_view()` method. Here's the final line from the `create_pos_view()` function:\n\n```python\n    final_df.create_or_replace_view('POS_FLATTENED_V')\n```\n\nFor more details about the Snowpark Python DataFrame API, please check out our [Working with DataFrames in Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes.html) page.\n\n### Streams for Incremental Processing (CDC)\nSnowflake makes processing data incrementally very easy. Traditionally the data engineer had to keep track of a high watermark (usually a datetime column) in order to process only new records in a table. This involved tracking and persisting that watermark somewhere and then using it in any query against the source table. But with Snowflake streams all the heavy lifting is done for you by Snowflake. For more details please check out our [Change Tracking Using Table Streams](https://docs.snowflake.com/en/user-guide/streams.html) user guide.\n\nAll you need to do is create a [`STREAM`](https://docs.snowflake.com/en/sql-reference/sql/create-stream.html) object in Snowflake against your base table or view, then query that stream just like any table in Snowflake. The stream will return only the changed records since the last DML option your performed. To help you work with the changed records, Snowflake streams will supply the following metadata columns along with the base table or view columns:\n\n* METADATA$ACTION\n* METADATA$ISUPDATE\n* METADATA$ROW_ID\n\nFor more details about these stream metadata columns please check out the [Stream Columns](https://docs.snowflake.com/en/user-guide/streams-intro.html#stream-columns) section in our documentation.\n\n### Streams on views\nWhat's really cool about Snowflake's incremental/CDC stream capability is the ability to create a stream on a view! In this example we are creating a stream on a view which joins together 6 of the raw POS tables. Here is the code to do that:\n\n```python\ndef create_pos_view_stream(session):\n    session.use_schema('HARMONIZED')\n    _ = session.sql('CREATE OR REPLACE STREAM POS_FLATTENED_V_STREAM \\\n                        ON VIEW POS_FLATTENED_V \\\n                        SHOW_INITIAL_ROWS = TRUE').collect()\n```\n\nNow when we query the `POS_FLATTENED_V_STREAM` stream to find changed records, Snowflake is actually looking for changed records in any of the 6 tables included in the view. For those who have tried to build incremental/CDC processes around denormalized schemas like this, you will appreciate the incredibly powerful feature that Snowflake provides here.\n\nFor more details please check out the [Streams on Views](https://docs.snowflake.com/en/user-guide/streams-intro.html#streams-on-views) section in our documentation.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell22"
      },
      "source": "## Fahrenheit to Celsius UDF\n\nDuring this step we will be creating and deploying our first Snowpark Python object to Snowflake, a user-defined function (or UDF). To begin with the UDF will be very basic, but in a future step we'll update it to include a third-party Python package. Also in this step you will be introduced to the new SnowCLI, a new developer command line tool. SnowCLI makes building and deploying Snowpark Python objects to Snowflake a consistent experience for the developer. More details below on SnowCLI. To put this in context, we are on step **#5** in our data flow overview.",
      "id": "33f62ff3-5016-4514-8571-ed6195353e07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE FUNCTION NB_HOL_DB.ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(\"temp_f\" FLOAT)\n",
        "RETURNS FLOAT\n",
        "LANGUAGE PYTHON\n",
        "RUNTIME_VERSION=3.8\n",
        "HANDLER = 'main'\n",
        "AS '\n",
        "def main(temp_f: float) -> float:\n",
        "    return (float(temp_f) - 32) * (5/9)\n",
        "';"
      ],
      "id": "a4f247e1-40d0-4003-92d5-3a19712254f1"
    },
    {
      "cell_type": "markdown",
      "id": "09e72e42-4bf6-454c-b76c-e323fcfdbca8",
      "metadata": {
        "name": "cell24"
      },
      "source": "### Running the UDF in Snowflake\nIn order to run the UDF in Snowflake you have a few options. Any UDF in Snowflake can be invoked through SQL as follows:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell25"
      },
      "outputs": [],
      "source": [
        "SELECT NB_HOL_DB.ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF(35);"
      ],
      "id": "070c5e8a-f1e7-4fde-9b8e-e3514e3856e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell26"
      },
      "source": "## Orders Update Sproc\n\nDuring this step we will be creating and deploying our first Snowpark Python stored procedure (or sproc) to Snowflake. This sproc will merge changes from the `HARMONIZED.POS_FLATTENED_V_STREAM` stream into our target `HARMONIZED.ORDERS` table. To put this in context, we are on step **#6** in our data flow overview.\n\n### Running the Sproc Locally\nTo test the procedure locally, you will execute the following script.",
      "id": "123bc413-3c14-4092-bc61-d6cc021704e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell27"
      },
      "outputs": [],
      "source": "# SNOWFLAKE ADVANTAGE: Python Stored Procedures\n\nimport time\nfrom snowflake.snowpark import Session\n#import snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_orders_table(session):\n    _ = session.sql(\"CREATE TABLE HARMONIZED.ORDERS LIKE HARMONIZED.POS_FLATTENED_V\").collect()\n    _ = session.sql(\"ALTER TABLE HARMONIZED.ORDERS ADD COLUMN META_UPDATED_AT TIMESTAMP\").collect()\n\ndef create_orders_stream(session):\n    _ = session.sql(\"CREATE STREAM HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS\").collect()\n\ndef merge_order_updates(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n    target = session.table('HARMONIZED.ORDERS')\n\n    # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n    cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    # merge into DIM_CUSTOMER\n    target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the ORDERS table and ORDERS_STREAM stream if they don't exist\n    if not table_exists(session, schema='HARMONIZED', name='ORDERS'):\n        create_orders_table(session)\n        create_orders_stream(session)\n\n    # Process data incrementally\n    merge_order_updates(session)\n#    session.table('HARMONIZED.ORDERS').limit(5).show()\n\n    return f\"Successfully processed ORDERS\"",
      "id": "2e75b295-bd82-4ed6-a565-259a56053c61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell28"
      },
      "outputs": [],
      "source": [
        "main(session)"
      ],
      "id": "f9faffa0-45a3-48e2-aa01-db9c512b0881"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell29"
      },
      "outputs": [],
      "source": [
        "script = '''\n",
        "import time\n",
        "from snowflake.snowpark import Session\n",
        "#import snowflake.snowpark.types as T\n",
        "import snowflake.snowpark.functions as F\n",
        "\n",
        "\n",
        "def table_exists(session, schema='', name=''):\n",
        "    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n",
        "    return exists\n",
        "\n",
        "def create_orders_table(session):\n",
        "    _ = session.sql(\"CREATE TABLE HARMONIZED.ORDERS LIKE HARMONIZED.POS_FLATTENED_V\").collect()\n",
        "    _ = session.sql(\"ALTER TABLE HARMONIZED.ORDERS ADD COLUMN META_UPDATED_AT TIMESTAMP\").collect()\n",
        "\n",
        "def create_orders_stream(session):\n",
        "    _ = session.sql(\"CREATE STREAM HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS\").collect()\n",
        "\n",
        "def merge_order_updates(session):\n",
        "    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n",
        "\n",
        "    source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n",
        "    target = session.table('HARMONIZED.ORDERS')\n",
        "\n",
        "    # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n",
        "    cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n",
        "    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n",
        "    updates = {**cols_to_update, **metadata_col_to_update}\n",
        "\n",
        "    # merge into DIM_CUSTOMER\n",
        "    target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n",
        "                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n",
        "\n",
        "    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n",
        "\n",
        "def main(session: Session) -> str:\n",
        "    # Create the ORDERS table and ORDERS_STREAM stream if they don't exist\n",
        "    if not table_exists(session, schema='HARMONIZED', name='ORDERS'):\n",
        "        create_orders_table(session)\n",
        "        create_orders_stream(session)\n",
        "\n",
        "    # Process data incrementally\n",
        "    merge_order_updates(session)\n",
        "#    session.table('HARMONIZED.ORDERS').limit(5).show()\n",
        "\n",
        "    return f\"Successfully processed ORDERS\"\n",
        "'''"
      ],
      "id": "680bb06b-273a-4bba-8155-a4180ec90245"
    },
    {
      "cell_type": "markdown",
      "id": "4de69c48-7dd4-4e15-bb2b-3cd46f8a5972",
      "metadata": {
        "name": "cell30"
      },
      "source": "Here is the SQL query to deploy the procedure:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell31"
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE PROCEDURE orders_update_sp()\n",
        " RETURNS string\n",
        " LANGUAGE PYTHON\n",
        " RUNTIME_VERSION=3.8\n",
        " PACKAGES=('snowflake-snowpark-python','toml')\n",
        " HANDLER = 'main'\n",
        " AS $$\n",
        " {{script}}\n",
        " $$;"
      ],
      "id": "597c8215-aa83-4041-b0d0-61918a81b250"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell32"
      },
      "outputs": [],
      "source": [
        "CALL ORDERS_UPDATE_SP();"
      ],
      "id": "edc7aacb-152a-4b53-b83a-a24578b27333"
    },
    {
      "cell_type": "markdown",
      "id": "46f5f6a9-334c-41be-b69f-72b0b2ff51bd",
      "metadata": {
        "name": "cell33"
      },
      "source": "### More on the Snowpark API\nIn this step we're starting to really use the Snowpark DataFrame API for data transformations. The Snowpark API provides the same functionality as the [Spark SQL API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html). To begin with you need to create a Snowpark session object. Like PySpark, this is accomplished with the `Session.builder.configs().create()` methods. When running locally, we use the `Session.builder.getOrCreate()` method to create the session object for us. But when deployed to Snowflake, the session object is provisioned for you automatically by Snowflake. And when building a Snowpark Python sproc the contract is that the first argument to the entry point (or handler) function is a Snowpark session.\n\nThe first thing you'll notice in the script is that we have some functions which use SQL to create objects in Snowflake and to check object status. To issue a SQL statement to Snowflake with the Snowpark API you use the `session.sql()` function, like you'd expect. Here's one example:\n\n```python\ndef create_orders_stream(session):\n    _ = session.sql(\"CREATE STREAM IF NOT EXISTS HARMONIZED.ORDERS_STREAM ON TABLE HARMONIZED.ORDERS \\\n                    SHOW_INITIAL_ROWS = TRUE;\").collect()\n```\n\nThe second thing to point out is how we're using DataFrames to merge changes from the source view to the target table. The Snowpark DataFrame API provides a `merge()` method which will ultimately generate a `MERGE` command in Snowflake.\n\n```python\n    source = session.table('HARMONIZED.POS_FLATTENED_V_STREAM')\n    target = session.table('HARMONIZED.ORDERS')\n\n    # TODO: Is the if clause supposed to be based on \"META_UPDATED_AT\"?\n    cols_to_update = {c: source[c] for c in source.schema.names if \"METADATA\" not in c}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    # merge into DIM_CUSTOMER\n    target.merge(source, target['ORDER_DETAIL_ID'] == source['ORDER_DETAIL_ID'], \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n```\n\nAgain, for more details about the Snowpark Python DataFrame API, please check out our [Working with DataFrames in Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes.html) page.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell34"
      },
      "source": "## Daily City Metrics Update Sproc\nDuring this step we will be creating and deploying our second Snowpark Python sproc to Snowflake. This sproc will join the `HARMONIZED.ORDERS` data with the Weather Source data to create a final, aggregated table for analysis named `ANALYTICS.DAILY_CITY_METRICS`. We will process the data incrementally from the `HARMONIZED.ORDERS` table using another Snowflake Stream. And we will again use the Snowpark DataFrame `merge()` method to merge/upsert the data. To put this in context, we are on step **#7** in our data flow overview.",
      "id": "fd90d2f2-a045-4425-8ad0-7533c7d7611c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell35"
      },
      "outputs": [],
      "source": "import time\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_daily_city_metrics_table(session):\n    SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                        T.StructField(\"CITY_NAME\", T.StringType()),\n                                        T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                        T.StructField(\"DAILY_SALES\", T.StringType()),\n                                        T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                        T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                        T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                    ]\n    DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n    DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n\n    dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                        .na.drop() \\\n                        .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n\n\ndef merge_daily_city_metrics(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    print(\"{} records in stream\".format(session.table('HARMONIZED.ORDERS_STREAM').count()))\n    orders_stream_dates = session.table('HARMONIZED.ORDERS_STREAM').select(F.col(\"ORDER_TS_DATE\").alias(\"DATE\")).distinct()\n    orders_stream_dates.limit(5).show()\n\n    orders = session.table(\"HARMONIZED.ORDERS_STREAM\").group_by(F.col('ORDER_TS_DATE'), F.col('PRIMARY_CITY'), F.col('COUNTRY')) \\\n                                        .agg(F.sum(F.col(\"PRICE\")).as_(\"price_nulls\")) \\\n                                        .with_column(\"DAILY_SALES\", F.call_builtin(\"ZEROIFNULL\", F.col(\"price_nulls\"))) \\\n                                        .select(F.col('ORDER_TS_DATE').alias(\"DATE\"), F.col(\"PRIMARY_CITY\").alias(\"CITY_NAME\"), \\\n                                        F.col(\"COUNTRY\").alias(\"COUNTRY_DESC\"), F.col(\"DAILY_SALES\"))\n#    orders.limit(5).show()\n\n    weather_pc = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES\")\n    countries = session.table(\"RAW_POS.COUNTRY\")\n    weather = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\n    weather = weather.join(weather_pc, (weather['POSTAL_CODE'] == weather_pc['POSTAL_CODE']) & (weather['COUNTRY'] == weather_pc['COUNTRY']), rsuffix='_pc')\n    weather = weather.join(countries, (weather['COUNTRY'] == countries['ISO_COUNTRY']) & (weather['CITY_NAME'] == countries['CITY']), rsuffix='_c')\n    weather = weather.join(orders_stream_dates, weather['DATE_VALID_STD'] == orders_stream_dates['DATE'])\n\n    weather_agg = weather.group_by(F.col('DATE_VALID_STD'), F.col('CITY_NAME'), F.col('COUNTRY_C')) \\\n                        .agg( \\\n                            F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF\", F.col(\"AVG_TEMPERATURE_AIR_2M_F\"))).alias(\"AVG_TEMPERATURE_C\"), \\\n                            F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.INCH_TO_MILLIMETER_UDF\", F.col(\"TOT_PRECIPITATION_IN\"))).alias(\"AVG_PRECIPITATION_MM\"), \\\n                            F.max(F.col(\"MAX_WIND_SPEED_100M_MPH\")).alias(\"MAX_WIND_SPEED_100M_MPH\") \\\n                        ) \\\n                        .select(F.col(\"DATE_VALID_STD\").alias(\"DATE\"), F.col(\"CITY_NAME\"), F.col(\"COUNTRY_C\").alias(\"COUNTRY_DESC\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_C\"), 2).alias(\"AVG_TEMPERATURE_CELSIUS\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_MM\"), 2).alias(\"AVG_PRECIPITATION_MILLIMETERS\"), \\\n                            F.col(\"MAX_WIND_SPEED_100M_MPH\")\n                            )\n#    weather_agg.limit(5).show()\n\n    daily_city_metrics_stg = orders.join(weather_agg, (orders['DATE'] == weather_agg['DATE']) & (orders['CITY_NAME'] == weather_agg['CITY_NAME']) & (orders['COUNTRY_DESC'] == weather_agg['COUNTRY_DESC']), \\\n                        how='left', rsuffix='_w') \\\n                    .select(\"DATE\", \"CITY_NAME\", \"COUNTRY_DESC\", \"DAILY_SALES\", \\\n                        \"AVG_TEMPERATURE_FAHRENHEIT\", \"AVG_TEMPERATURE_CELSIUS\", \\\n                        \"AVG_PRECIPITATION_INCHES\", \"AVG_PRECIPITATION_MILLIMETERS\", \\\n                        \"MAX_WIND_SPEED_100M_MPH\")\n#    daily_city_metrics_stg.limit(5).show()\n\n    cols_to_update = {c: daily_city_metrics_stg[c] for c in daily_city_metrics_stg.schema.names}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm.merge(daily_city_metrics_stg, (dcm['DATE'] == daily_city_metrics_stg['DATE']) & (dcm['CITY_NAME'] == daily_city_metrics_stg['CITY_NAME']) & (dcm['COUNTRY_DESC'] == daily_city_metrics_stg['COUNTRY_DESC']), \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the DAILY_CITY_METRICS table if it doesn't exist\n    if not table_exists(session, schema='ANALYTICS', name='DAILY_CITY_METRICS'):\n        create_daily_city_metrics_table(session)\n    \n    merge_daily_city_metrics(session)\n#    session.table('ANALYTICS.DAILY_CITY_METRICS').limit(5).show()\n\n    return f\"Successfully processed DAILY_CITY_METRICS\"",
      "id": "954fa22c-f646-4ab8-aea4-6ac17e5b96e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "python",
        "name": "cell36"
      },
      "outputs": [],
      "source": [
        "main(session)"
      ],
      "id": "1ddce8d6-8d33-4ec5-ba24-6ce352f3ee0e"
    },
    {
      "cell_type": "markdown",
      "id": "cfadb325-1f5a-4ce5-baef-8643bb22dd03",
      "metadata": {
        "name": "cell37",
        "collapsed": false
      },
      "source": "### Deploying the Sproc to Snowflake\nTo deploy your sproc to Snowflake by running the following commands:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell38"
      },
      "outputs": [],
      "source": "script = '''\nimport time\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.types as T\nimport snowflake.snowpark.functions as F\n\n\ndef table_exists(session, schema='', name=''):\n    exists = session.sql(\"SELECT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{}' AND TABLE_NAME = '{}') AS TABLE_EXISTS\".format(schema, name)).collect()[0]['TABLE_EXISTS']\n    return exists\n\ndef create_daily_city_metrics_table(session):\n    SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                        T.StructField(\"CITY_NAME\", T.StringType()),\n                                        T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                        T.StructField(\"DAILY_SALES\", T.StringType()),\n                                        T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                        T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                        T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                    ]\n    DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n    DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n\n    dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                        .na.drop() \\\n                        .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n\n\ndef merge_daily_city_metrics(session):\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE').collect()\n\n    print(\"{} records in stream\".format(session.table('HARMONIZED.ORDERS_STREAM').count()))\n    orders_stream_dates = session.table('HARMONIZED.ORDERS_STREAM').select(F.col(\"ORDER_TS_DATE\").alias(\"DATE\")).distinct()\n    orders_stream_dates.limit(5).show()\n\n    orders = session.table(\"HARMONIZED.ORDERS_STREAM\").group_by(F.col('ORDER_TS_DATE'), F.col('PRIMARY_CITY'), F.col('COUNTRY')) \\\n                                        .agg(F.sum(F.col(\"PRICE\")).as_(\"price_nulls\")) \\\n                                        .with_column(\"DAILY_SALES\", F.call_builtin(\"ZEROIFNULL\", F.col(\"price_nulls\"))) \\\n                                        .select(F.col('ORDER_TS_DATE').alias(\"DATE\"), F.col(\"PRIMARY_CITY\").alias(\"CITY_NAME\"), \\\n                                        F.col(\"COUNTRY\").alias(\"COUNTRY_DESC\"), F.col(\"DAILY_SALES\"))\n#    orders.limit(5).show()\n\n    weather_pc = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES\")\n    countries = session.table(\"RAW_POS.COUNTRY\")\n    weather = session.table(\"FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.HISTORY_DAY\")\n    weather = weather.join(weather_pc, (weather['POSTAL_CODE'] == weather_pc['POSTAL_CODE']) & (weather['COUNTRY'] == weather_pc['COUNTRY']), rsuffix='_pc')\n    weather = weather.join(countries, (weather['COUNTRY'] == countries['ISO_COUNTRY']) & (weather['CITY_NAME'] == countries['CITY']), rsuffix='_c')\n    weather = weather.join(orders_stream_dates, weather['DATE_VALID_STD'] == orders_stream_dates['DATE'])\n\n    weather_agg = weather.group_by(F.col('DATE_VALID_STD'), F.col('CITY_NAME'), F.col('COUNTRY_C')) \\\n                        .agg( \\\n                            F.avg('AVG_TEMPERATURE_AIR_2M_F').alias(\"AVG_TEMPERATURE_F\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.FAHRENHEIT_TO_CELSIUS_UDF\", F.col(\"AVG_TEMPERATURE_AIR_2M_F\"))).alias(\"AVG_TEMPERATURE_C\"), \\\n                            F.avg(\"TOT_PRECIPITATION_IN\").alias(\"AVG_PRECIPITATION_IN\"), \\\n                            F.avg(F.call_udf(\"ANALYTICS.INCH_TO_MILLIMETER_UDF\", F.col(\"TOT_PRECIPITATION_IN\"))).alias(\"AVG_PRECIPITATION_MM\"), \\\n                            F.max(F.col(\"MAX_WIND_SPEED_100M_MPH\")).alias(\"MAX_WIND_SPEED_100M_MPH\") \\\n                        ) \\\n                        .select(F.col(\"DATE_VALID_STD\").alias(\"DATE\"), F.col(\"CITY_NAME\"), F.col(\"COUNTRY_C\").alias(\"COUNTRY_DESC\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_F\"), 2).alias(\"AVG_TEMPERATURE_FAHRENHEIT\"), \\\n                            F.round(F.col(\"AVG_TEMPERATURE_C\"), 2).alias(\"AVG_TEMPERATURE_CELSIUS\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_IN\"), 2).alias(\"AVG_PRECIPITATION_INCHES\"), \\\n                            F.round(F.col(\"AVG_PRECIPITATION_MM\"), 2).alias(\"AVG_PRECIPITATION_MILLIMETERS\"), \\\n                            F.col(\"MAX_WIND_SPEED_100M_MPH\")\n                            )\n#    weather_agg.limit(5).show()\n\n    daily_city_metrics_stg = orders.join(weather_agg, (orders['DATE'] == weather_agg['DATE']) & (orders['CITY_NAME'] == weather_agg['CITY_NAME']) & (orders['COUNTRY_DESC'] == weather_agg['COUNTRY_DESC']), \\\n                        how='left', rsuffix='_w') \\\n                    .select(\"DATE\", \"CITY_NAME\", \"COUNTRY_DESC\", \"DAILY_SALES\", \\\n                        \"AVG_TEMPERATURE_FAHRENHEIT\", \"AVG_TEMPERATURE_CELSIUS\", \\\n                        \"AVG_PRECIPITATION_INCHES\", \"AVG_PRECIPITATION_MILLIMETERS\", \\\n                        \"MAX_WIND_SPEED_100M_MPH\")\n#    daily_city_metrics_stg.limit(5).show()\n\n    cols_to_update = {c: daily_city_metrics_stg[c] for c in daily_city_metrics_stg.schema.names}\n    metadata_col_to_update = {\"META_UPDATED_AT\": F.current_timestamp()}\n    updates = {**cols_to_update, **metadata_col_to_update}\n\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm.merge(daily_city_metrics_stg, (dcm['DATE'] == daily_city_metrics_stg['DATE']) & (dcm['CITY_NAME'] == daily_city_metrics_stg['CITY_NAME']) & (dcm['COUNTRY_DESC'] == daily_city_metrics_stg['COUNTRY_DESC']), \\\n                        [F.when_matched().update(updates), F.when_not_matched().insert(updates)])\n\n    _ = session.sql('ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL').collect()\n\ndef main(session: Session) -> str:\n    # Create the DAILY_CITY_METRICS table if it doesn't exist\n    if not table_exists(session, schema='ANALYTICS', name='DAILY_CITY_METRICS'):\n        create_daily_city_metrics_table(session)\n    \n    merge_daily_city_metrics(session)\n#    session.table('ANALYTICS.DAILY_CITY_METRICS').limit(5).show()\n\n    return f\"Successfully processed DAILY_CITY_METRICS\"\n'''",
      "id": "04c908df-3f1c-49ae-97e5-b89e159bd44b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell39"
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE PROCEDURE DAILY_CITY_METRICS_UPDATE_SP()\n",
        " RETURNS string\n",
        " LANGUAGE PYTHON\n",
        " RUNTIME_VERSION=3.8\n",
        " PACKAGES=('snowflake-snowpark-python','toml')\n",
        " HANDLER = 'main'\n",
        " AS $$\n",
        " {{script}}\n",
        " $$;"
      ],
      "id": "02b166f6-97d8-4d08-8c41-f49c0aae1aec"
    },
    {
      "cell_type": "markdown",
      "id": "b9446717-c53a-41f2-9a80-6ebc5548ad9c",
      "metadata": {
        "name": "cell40"
      },
      "source": "### Running the Sproc in Snowflake\nIn order to run the sproc in Snowflake you have a few options. Any sproc in Snowflake can be invoked through SQL as follows:\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell41"
      },
      "outputs": [],
      "source": [
        "CALL DAILY_CITY_METRICS_UPDATE_SP();"
      ],
      "id": "16e5289e-5197-4bda-ac42-551c609787b5"
    },
    {
      "cell_type": "markdown",
      "id": "59c9f51b-0ba8-40ee-9524-a52086cafc56",
      "metadata": {
        "name": "cell42"
      },
      "source": "### Data Modeling Best Practice\nWhen modeling data for analysis a best practice has been to clearly define and manage the schema of the table. In step 2, when we loaded raw data from Parquet we took advantage of Snowflake's schema detection feature to create a table with the same schema as the Parquet files. In this step we are explicitly defining the schema in DataFrame syntax and using that to create the table.\n\n```python\ndef create_daily_city_metrics_table(session):\n    SHARED_COLUMNS= [T.StructField(\"DATE\", T.DateType()),\n                                        T.StructField(\"CITY_NAME\", T.StringType()),\n                                        T.StructField(\"COUNTRY_DESC\", T.StringType()),\n                                        T.StructField(\"DAILY_SALES\", T.StringType()),\n                                        T.StructField(\"AVG_TEMPERATURE_FAHRENHEIT\", T.DecimalType()),\n                                        T.StructField(\"AVG_TEMPERATURE_CELSIUS\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_INCHES\", T.DecimalType()),\n                                        T.StructField(\"AVG_PRECIPITATION_MILLIMETERS\", T.DecimalType()),\n                                        T.StructField(\"MAX_WIND_SPEED_100M_MPH\", T.DecimalType()),\n                                    ]\n    DAILY_CITY_METRICS_COLUMNS = [*SHARED_COLUMNS, T.StructField(\"META_UPDATED_AT\", T.TimestampType())]\n    DAILY_CITY_METRICS_SCHEMA = T.StructType(DAILY_CITY_METRICS_COLUMNS)\n\n    dcm = session.create_dataframe([[None]*len(DAILY_CITY_METRICS_SCHEMA.names)], schema=DAILY_CITY_METRICS_SCHEMA) \\\n                        .na.drop() \\\n                        .write.mode('overwrite').save_as_table('ANALYTICS.DAILY_CITY_METRICS')\n    dcm = session.table('ANALYTICS.DAILY_CITY_METRICS')\n```\n\n### Complex Aggregation Query\nThe `merge_daily_city_metrics()` function contains a complex aggregation query which is used to join together and aggregate the data from our POS and Weather Source. Take a look at the series of complex series of joins and aggregations that are expressed, and how we're even leveraging the Snowpark UDF we created in step #5!\n\nThe complex aggregation query is then merged into the final analytics table using the Snowpark `merge()` method. If you haven't already, check out your Snowflake Query history and see which queries were generated by the Snowpark API. In this case you will see that the Snowpark API took all the complex logic, including the merge and created a single Snowflake query to execute!\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell43"
      },
      "source": "# Orchestrate Jobs\n\nDuring this step we will be orchestrating our new Snowpark pipelines with Snowflake's native orchestration feature named Tasks. We will create two tasks, one for each stored procedure, and chain them together. We will then run the tasks. To put this in context, we are on step **#8** in our data flow overview.\n",
      "id": "3e8c2c63-c904-46aa-b0e4-dddaa502d5a4"
    },
    {
      "cell_type": "markdown",
      "id": "d5b5b07a-cf8b-414b-9308-e1668ed57746",
      "metadata": {
        "name": "cell44"
      },
      "source": "### Run the Script"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "language": "sql",
        "name": "cell45"
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Step #1: Create the tasks to call our Python stored procedures\n-- ----------------------------------------------------------------------------\n\nCREATE OR REPLACE TASK ORDERS_UPDATE_TASK\nWAREHOUSE = HOL_WH\nWHEN\n  SYSTEM$STREAM_HAS_DATA('POS_FLATTENED_V_STREAM')\nAS\nCALL HARMONIZED.ORDERS_UPDATE_SP();\n\nCREATE OR REPLACE TASK DAILY_CITY_METRICS_UPDATE_TASK\nWAREHOUSE = HOL_WH\nAFTER ORDERS_UPDATE_TASK\nWHEN\n  SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')\nAS\nCALL ANALYTICS.DAILY_CITY_METRICS_UPDATE_SP();\n\n-- ----------------------------------------------------------------------------\n-- Step #2: Execute the tasks\n-- ----------------------------------------------------------------------------\n\nALTER TASK DAILY_CITY_METRICS_UPDATE_TASK RESUME;\n\nEXECUTE TASK ORDERS_UPDATE_TASK;",
      "id": "868473c7-aea4-430a-be35-7f710fc34729"
    },
    {
      "cell_type": "markdown",
      "id": "249ba473-5fb5-4ecd-a99c-99c1dcc10f0e",
      "metadata": {
        "name": "cell46",
        "collapsed": false
      },
      "source": "### Running the Tasks\nIn this step we did not create a schedule for our task DAG, so it will not run on its own at this point. So in this script you will notice that we manually execute the DAG, like this:\n\n```sql\nEXECUTE TASK ORDERS_UPDATE_TASK;\n```\n\nTo see what happened when you ran this task just now, highlight and run (using CMD/CTRL+Enter) this commented query in the script:\n\n```sql\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n    SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n    RESULT_LIMIT => 100))\nORDER BY SCHEDULED_TIME DESC\n;\n```\n\nYou will notice in the task history output that it skipped our task `ORDERS_UPDATE_TASK`. This is correct, because our `HARMONIZED.POS_FLATTENED_V_STREAM` stream doesn't have any data. We'll add some new data and run them again in the next step.\n\n### More on Tasks\nTasks are Snowflake's native scheduling/orchestration feature. With a task you can execute any one of the following types of SQL code:\n\n* Single SQL statement\n* Call to a stored procedure\n* Procedural logic using Snowflake Scripting Developer Guide\n\nFor this Quickstart we'll call our Snowpark stored procedures. Here is the SQL DDL code to create the second task:\n\n```sql\nCREATE OR REPLACE TASK DAILY_CITY_METRICS_UPDATE_TASK\nWAREHOUSE = HOL_WH\nAFTER ORDERS_UPDATE_TASK\nWHEN\n  SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')\nAS\nCALL ANALYTICS.DAILY_CITY_METRICS_UPDATE_SP();\n```\n\nA few things to point out. First you specify which Snowflake virtual warehouse to use when running the task with the `WAREHOUSE` clause. The `AFTER` clause lets you define the relationship between tasks, and the structure of this relationship is a Directed Acyclic Graph (or DAG) like most orchestration tools provide. The `AS` clause let's you define what the task should do when it runs, in this case to call our stored procedure.\n\nThe `WHEN` clause is really cool. We've already seen how streams work in Snowflake by allowing you to incrementally process data. We've even seen how you can create a stream on a view (which joins many tables together) and create a stream on that view to process its data incrementally! Here in the `WHEN` clause we're calling a system function `SYSTEM$STREAM_HAS_DATA()` which returns true if the specified stream has new data. With the `WHEN` clause in place the virtual warehouse will only be started up when the stream has new data. So if there's no new data when the task runs then your warehouse won't be started up and you won't be charged. You will only be charged when there's new data to process. Pretty cool, huh?\n\nAs mentioned above we did not define a `SCHEDULE` for the root task, so this DAG will not run on its own. That's fine for this Quickstart, but in a real situation you would define a schedule. See [CREATE TASK](https://docs.snowflake.com/en/sql-reference/sql/create-task.html) for the details.\n\nAnd for more details on Tasks see [Introduction to Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro.html).\n\n### Task Metadata\nSnowflake keeps metadata for almost everything you do, and makes that metadata available for you to query (and to create any type of process around). Tasks are no different, Snowflake maintains rich metadata to help you monitor your task runs. Here are a few sample SQL queries you can use to monitor your tasks runs:\n\n```sql\n-- Get a list of tasks\nSHOW TASKS;\n\n-- Task execution history in the past day\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n    SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n    RESULT_LIMIT => 100))\nORDER BY SCHEDULED_TIME DESC\n;\n\n-- Scheduled task runs\nSELECT\n    TIMESTAMPDIFF(SECOND, CURRENT_TIMESTAMP, SCHEDULED_TIME) NEXT_RUN,\n    SCHEDULED_TIME,\n    NAME,\n    STATE\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\nWHERE STATE = 'SCHEDULED'\nORDER BY COMPLETED_TIME DESC;\n```\n\n### Monitoring Tasks\nSo while you're free to create any operational or monitoring process you wish, Snowflake provides some rich task observability features in our Snowsight UI. Try it out for yourself by following these steps:\n\n1. In the Snowsight navigation menu, click **Data** \u00bb **Databases**.\n1. In the right pane, using the object explorer, navigate to a database and schema.\n1. For the selected schema, select and expand **Tasks**.\n1. Select a task. Task information is displayed, including **Task Details**, **Graph**, and **Run History** sub-tabs.\n1. Select the **Graph** tab. The task graph appears, displaying a hierarchy of child tasks.\n1. Select a task to view its details."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell47"
      },
      "source": "# Process Incrementally\n\nDuring this step we will be adding new data to our POS order tables and then running our entire end-to-end pipeline to process the new data. And this entire pipeline will be processing data incrementally thanks to Snowflake's advanced stream/CDC capabilities. To put this in context, we are on step **#9** in our data flow overview.\n",
      "id": "0439490a-61bd-45cd-b99b-357de4c86efa"
    },
    {
      "cell_type": "code",
      "id": "c8084fdd-ad00-4d02-b762-bbd076d17bd3",
      "metadata": {
        "language": "sql",
        "name": "cell48",
        "collapsed": false,
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "-- ----------------------------------------------------------------------------\n-- Step #1: Add new/remaining order data\n-- ----------------------------------------------------------------------------\n\nUSE SCHEMA RAW_POS;\n\nALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XLARGE WAIT_FOR_COMPLETION = TRUE;\n\nLS @external.frostbyte_raw_stage/pos/order_header/year=2022",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell49",
        "collapsed": false
      },
      "outputs": [],
      "source": "COPY INTO ORDER_HEADER FROM @external.frostbyte_raw_stage/pos/order_header/year=2022\nFILE_FORMAT = (FORMAT_NAME = EXTERNAL.PARQUET_FORMAT)\nMATCH_BY_COLUMN_NAME = CASE_SENSITIVE;",
      "id": "e7438f98-90f1-4a50-bbd8-832ce084a440"
    },
    {
      "cell_type": "code",
      "id": "62ac79c7-a351-41e2-b80a-96c2d899c363",
      "metadata": {
        "language": "sql",
        "name": "cell50"
      },
      "outputs": [],
      "source": "COPY INTO ORDER_DETAIL FROM @external.frostbyte_raw_stage/pos/order_detail/year=2022\nFILE_FORMAT = (FORMAT_NAME = EXTERNAL.PARQUET_FORMAT)\nMATCH_BY_COLUMN_NAME = CASE_SENSITIVE;",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "6c8b168c-c5c0-4af7-86f4-7ade69949283",
      "metadata": {
        "language": "sql",
        "name": "cell51"
      },
      "outputs": [],
      "source": "ALTER WAREHOUSE HOL_WH SET WAREHOUSE_SIZE = XSMALL;",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "5009d825-7703-4370-a26a-072aaf7a8a56",
      "metadata": {
        "name": "cell52"
      },
      "source": "### Viewing the Task History\nLike the in the previous step, to see what happened when you ran this task DAG, highlight and run (using CMD/CTRL+Enter) this commented query in the script:\n\n```sql\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n    SCHEDULED_TIME_RANGE_START=>DATEADD('DAY',-1,CURRENT_TIMESTAMP()),\n    RESULT_LIMIT => 100))\nORDER BY SCHEDULED_TIME DESC\n;\n```\n\nThis time you will notice that the `ORDERS_UPDATE_TASK` task will not be skipped, since the `HARMONIZED.POS_FLATTENED_V_STREAM` stream has new data. In a few minutes you should see that both the `ORDERS_UPDATE_TASK` task and the `DAILY_CITY_METRICS_UPDATE_TASK` task completed successfully.\n\n### Query History for Tasks\nOne important thing to understand about tasks, is that the queries which get executed by the task won't show up with the default Query History UI settings. In order to see the queries that just ran you need to do the following:\n\n* Remove filters at the top of this table, including your username, as later scheduled tasks will run as \"System\":\n\n![](assets/query_history_remove_filter1.png)\n\n* Click \"Filter\", and add filter option 'Queries executed by user tasks' and click \"Apply Filters\":\n\n![](assets/query_history_remove_filter2.png)\n\nYou should now see all the queries run by your tasks! Take a look at each of the MERGE commands in the Query History to see how many records were processed by each task. And don't forget to notice that we processed the whole pipeline just now, and did so incrementally!\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "name": "cell53"
      },
      "source": "## Teardown\nOnce you're finished with the Quickstart and want to clean things up, you can simply run the following commands.\n",
      "id": "ef47cbae-164e-48b2-8ab0-fafb6b4b09ab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "codeCollapsed": false,
        "language": "sql",
        "name": "cell54",
        "collapsed": false
      },
      "outputs": [],
      "source": "DROP DATABASE NB_HOL_DB;\nDROP WAREHOUSE HOL_WH;\nDROP ROLE HOL_ROLE;\nDROP DATABASE FROSTBYTE_WEATHERSOURCE;",
      "id": "bec5625e-83a3-405b-a78b-07d82088ef7e"
    },
    {
      "cell_type": "markdown",
      "id": "bb371043-ba33-4d70-bc49-e388162c6e82",
      "metadata": {
        "name": "cell55"
      },
      "source": "### What we've covered\nWe've covered a ton in this Quickstart, and here are the highlights:\n\n* Snowflake's Table Format\n* Data ingestion with COPY\n* Schema inference\n* Data sharing/marketplace (instead of ETL)\n* Streams for incremental processing (CDC)\n* Streams on views\n* Python UDFs (with third-party packages)\n* Python Stored Procedures\n* Snowpark DataFrame API\n* Snowpark Python programmability\n* Warehouse elasticity (dynamic scaling)\n* Visual Studio Code Snowflake native extension (PuPr, Git integration)\n* SnowCLI (PuPr)\n* Tasks (with Stream triggers)\n* Task Observability\n* GitHub Actions (CI/CD) integration\n\n### Related Resources\nAnd finally, here's a quick recap of related resources:\n\n* [Full Demo on Snowflake Demo Hub](https://developers.snowflake.com/demos/data-engineering-pipelines/)\n* [Source Code on GitHub](https://github.com/Snowflake-Labs/sfguide-data-engineering-with-snowpark-python)\n* [Snowpark Developer Guide for Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/index.html)\n    * [Writing Python UDFs](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python.html)\n    * [Writing Stored Procedures in Snowpark (Python)](https://docs.snowflake.com/en/sql-reference/stored-procedures-python.html)\n    * [Working with DataFrames in Snowpark Python](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes.html)\n* Related Tools\n    * [Snowflake Visual Studio Code Extension](https://marketplace.visualstudio.com/items?itemName=snowflake.snowflake-vsc)\n    * [SnowCLI Tool](https://github.com/Snowflake-Labs/snowcli)"
    }
  ]
}