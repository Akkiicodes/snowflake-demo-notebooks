{
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d0725965-00f2-4c63-9bd6-b0827b004907",
      "metadata": {
        "name": "cell1",
        "collapsed": false
      },
      "source": "# Setting up External Access for Snowflake Notebooks.\n\nBy default, Snowflake restricts network traffic from requests from public IP addresses. In order to access external endpoints, we first need to create an [external access integration](https://docs.snowflake.com/en/developer-guide/external-network-access/creating-using-external-network-access#label-creating-using-external-access-integration-access-integration). This example demonstrates how you can access an external endpoint to work with external services in your Snowflake Notebooks.\n\n\n**Requirements:** \n- Running this notebook require that you have ACCOUNTADMIN or SECURITYADMIN roles to create new network rules.\n- Please add the `transformers` and `pytorch` package from the package picker on the top right. We will be using these packages in the notebook."
    },
    {
      "cell_type": "markdown",
      "id": "133628b6-7c04-4f80-95a2-86bc1335c826",
      "metadata": {
        "name": "cell2",
        "collapsed": false
      },
      "source": "To access the external access configuration UI, click on the `\u22ee` button on the top right, then navigate to `Notebook settings` and the `External access` pane. Notebooks comes preconfigured with a set of commonly-used network rules for data science and machine learning. These may be pre-configured by your organization admin. \n"
    },
    {
      "cell_type": "markdown",
      "id": "318a98e5-17ad-49bf-aa0f-3bad3fd5e48d",
      "metadata": {
        "name": "cell3",
        "collapsed": false
      },
      "source": "## Example 1: Load sample dataset from Github\n\nLet say that you are trying to load a sample CSV file from a public Github repo.\n```python\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/altair-viz/vega_datasets/master/vega_datasets/_data/stocks.csv\")\n```\n\nWithout the external access setup, you will get the following error:\n\n```\nURLError: <urlopen error [Errno 16] Device or resource busy>\n```\n\nTo resolve this issue, you can toggle the `GH_ACCESS_INTEGRATION` on if it is available in your external access setup UI. \n\nIf the Github access integration is not available on your UI, run the following SQL *in a separate SQL worksheet* to create external access integration and network rule and attach it to the notebook.\n\n```sql\n-- Create the Github external access integration and the network rule it relies on.\nCREATE OR REPLACE NETWORK RULE gh_network_rule\n  MODE = EGRESS\n  TYPE = HOST_PORT\n  VALUE_LIST = ('raw.githubusercontent.com','githubusercontent.com','github.com');\n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION gh_access_integration\n  ALLOWED_NETWORK_RULES = (gh_network_rule)\n  ENABLED = true;\n```\n\nOnce you run the commands in the SQL worksheet, make sure to restart the notebook session. Now, you should see the `GH_ACCESS_INTEGRATION` in the external access UI and the following code can now run without error."
    },
    {
      "cell_type": "code",
      "id": "04524527-df7e-4d10-879b-a0fd417acd04",
      "metadata": {
        "language": "python",
        "name": "cell4",
        "codeCollapsed": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "import pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/altair-viz/vega_datasets/master/vega_datasets/_data/stocks.csv\")",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "386930fa-7038-4f99-ab3d-bcdff76b45c4",
      "metadata": {
        "name": "cell5",
        "collapsed": false
      },
      "source": "## Example 2: Accessing a pre-trained model from HuggingFace\n\nLet say that you are use a pre-trained model from HuggingFace to perform some NLP tasks.\n```python\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('Snowflake/snowflake-arctic-embed-xs')\nmodel = AutoModel.from_pretrained('Snowflake/snowflake-arctic-embed-xs', add_pooling_layer=False)\n```\n\nWithout the external access setup, you will get the following error:\n\n```\nOSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like Snowflake/snowflake-arctic-embed-xs is not the path to a directory containing a file named config.json. Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n```\n\nTo resolve this issue, you can toggle the `HF_ACCESS_INTEGRATION` on if it is available in your external access setup UI. \n\nIf the HuggingFace access integration is not available on your UI, run the following SQL *in a separate SQL worksheet* to create external access integration and network rule and attach it to the notebook.\n\n```sql\n-- Create the HuggingFace external access integration and the network rule it relies on.\nCREATE OR REPLACE NETWORK RULE hf_network_rule\n  MODE = EGRESS\n  TYPE = HOST_PORT\n  VALUE_LIST = ('huggingface.co');\n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION hf_access_integration\n  ALLOWED_NETWORK_RULES = (hf_network_rule)\n  ENABLED = true;\n```\n\nOnce you run the commands in the SQL worksheet, make sure to restart the notebook session. \n\nNow, you should see the `HF_ACCESS_INTEGRATION` in the external access UI and the following code can now run without error."
    },
    {
      "cell_type": "code",
      "id": "59eb42df-fe08-4a91-bf80-29975c8c0f57",
      "metadata": {
        "language": "python",
        "name": "cell6",
        "collapsed": false,
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "import os\nos.environ['HF_HOME'] = '/tmp/'",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "3775908f-ca36-4846-8f38-5adca39217f2",
      "metadata": {
        "language": "python",
        "name": "cell7",
        "codeCollapsed": false,
        "collapsed": false
      },
      "source": "import torch\nfrom transformers import AutoModel, AutoTokenizer\n# Load the pre-trained model (https://huggingface.co/Snowflake/snowflake-arctic-embed-xs)\ntokenizer = AutoTokenizer.from_pretrained('Snowflake/snowflake-arctic-embed-xs')\nmodel = AutoModel.from_pretrained('Snowflake/snowflake-arctic-embed-xs', add_pooling_layer=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "bc20f76e-dfc4-4bc9-a68d-abb39951be9e",
      "metadata": {
        "language": "python",
        "name": "cell8"
      },
      "outputs": [],
      "source": "# Set the model in evaluation model\nmodel.eval()",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "62b29ba4-df9f-4504-a04d-5252872202a5",
      "metadata": {
        "name": "cell9",
        "collapsed": false
      },
      "source": "Now that we have the pre-trained model, we can use it to score the relevance of the document given the query."
    },
    {
      "cell_type": "code",
      "id": "db9637a5-0ed9-4e24-b117-54ab1487910c",
      "metadata": {
        "language": "python",
        "name": "cell10",
        "collapsed": false,
        "codeCollapsed": false
      },
      "outputs": [],
      "source": "query_prefix = 'Represent this sentence for searching relevant passages: '\nqueries  = ['What is Snowflake?', 'Where can I get the best tacos?']\nqueries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\nquery_tokens = tokenizer(queries_with_prefix, padding=True, truncation=True, return_tensors='pt', max_length=512)\n\ndocuments = ['The Data Cloud!', 'Mexico City, of course!']\ndocument_tokens =  tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=512)\n\n# Compute token embeddings\nwith torch.no_grad():\n    query_embeddings = model(**query_tokens)[0][:, 0]\n    doument_embeddings = model(**document_tokens)[0][:, 0]\n\n\n# normalize embeddings\nquery_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\ndoument_embeddings = torch.nn.functional.normalize(doument_embeddings, p=2, dim=1)\n\nscores = torch.mm(query_embeddings, doument_embeddings.transpose(0, 1))\nfor query, query_scores in zip(queries, scores):\n    doc_score_pairs = list(zip(documents, query_scores))\n    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n    #Output passages & scores\n    print(\"Query:\", query)\n    for document, score in doc_score_pairs:\n        print(score, document)",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "fac4d31c-1b7a-462c-bfdc-a53f3021db8a",
      "metadata": {
        "name": "cell11",
        "collapsed": false
      },
      "source": "### Conclusion\n\n- In this example, we demonstrated how you set up external access integration to access a public endpoint. To load data from an external endpoint, you can also create an external access integration and attach it to a UDF. \n- Check out [this tutorial](https://github.com/Snowflake-Labs/snowflake-demo-notebooks/blob/main/Ingest%20Public%20JSON/Ingest%20Public%20JSON.ipynb) to learn how you can load semi-structured JSON data from a public endpoint into a Snowflake table.\n- To learn more about external network access to Snowflake, refer to the documentation [here](https://docs.snowflake.com/en/developer-guide/external-network-access/external-network-access-overview)."
    }
  ]
}